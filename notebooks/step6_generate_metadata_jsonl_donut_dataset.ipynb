{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script to create the metadata.jsonl file for donut training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "postqc = pd.read_csv('/home/balajic/workdir/donut_training_pipeline/donut_finetuning/database/postqc_dump.csv') # path to postqc file\n",
    "# postqc_qc_pass = pd.read_csv('/home/balajic/workdir/donut_training_pipeline/database/qc_pass_postqc_dump.csv') # path to postqc file\n",
    "# postqc_ext_fin = pd.read_csv('/home/balajic/workdir/donut_training_pipeline/database/extraction_finished_postqc_dump.csv') # path to postqc file\n",
    "\n",
    "# concatenate both the postqc into one biug file\n",
    "# postqc = pd.concat([postqc_qc_pass, postqc_ext_fin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['claim_id', 'current_state', 'claim_amount', 'txn_id', 'id', 'page_id',\n",
       "       'line_id', 'particular', 'unit_price', 'quantity', 'bd_amount',\n",
       "       'discount', 'ad_amount', 'l1', 'l2', 'nme', 'request_id', 'table_type',\n",
       "       'after_discount_amount_score', 'before_discount_amount_score',\n",
       "       'discount_score', 'particular_score', 'quantity_score', 'row_score',\n",
       "       'unit_price_score', 'sequence_id', 'bl_x', 'bl_y', 'br_x', 'br_y',\n",
       "       'tabulated_box', 'tl_x', 'tl_y', 'tr_x', 'tr_y', 'subtotal_hex_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postqc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>particular</th>\n",
       "      <th>bd_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28440</th>\n",
       "      <td>day care- room rent</td>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28821</th>\n",
       "      <td>surgeon fee</td>\n",
       "      <td>12000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28828</th>\n",
       "      <td>iol charges</td>\n",
       "      <td>95000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28829</th>\n",
       "      <td>anaesthesia charges</td>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28830</th>\n",
       "      <td>ot charges</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 particular  bd_amount\n",
       "28440  day care- room rent      3000.0\n",
       "28821           surgeon fee    12000.0\n",
       "28828           iol charges    95000.0\n",
       "28829   anaesthesia charges     3000.0\n",
       "28830            ot charges     2000.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postqc[postqc['claim_id']==22620558][['particular', 'bd_amount']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# postqc.columns\n",
    "# ignore pandas SettingWithCopyWarning\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 987/987 [00:01<00:00, 631.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation images processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "# import pytesseract\n",
    "\n",
    "# root path to the dataset contains the train, test and validation folders\n",
    "dataset_path = '/home/balajic/workdir/donut_training_pipeline/multipager_donut1/'\n",
    "interim_df_folder = '/home/balajic/workdir/donut_training_pipeline/donut_finetuning/interim_df/'\n",
    "save_path = '/home/balajic/workdir/donut_training_pipeline/'\n",
    "\n",
    "def process_image(image):\n",
    "    page_flag = False\n",
    "    \n",
    "    try:\n",
    "        # images are saved in the format of <claim_id>-<txn_id>-page_<page_id>.jpeg\n",
    "        claim = image.split('-')[0]\n",
    "        imgname = image.split('-')[-1]\n",
    "        txn_id = image.replace(f'-{imgname}','').replace(f'{claim}-','')\n",
    "        img_path = os.path.join(dataset_path, splitt, image)\n",
    "        img = Image.open(img_path)\n",
    "        width, height = img.size\n",
    "        \n",
    "        # read the intermediate df of bill_extractor\n",
    "        interim_df_path = os.path.join(interim_df_folder, image + '.csv')\n",
    "        if os.path.exists(interim_df_path):\n",
    "            df = pd.read_csv(interim_df_path)\n",
    "\n",
    "            df.replace(\"VNA\",np.nan,inplace=True)\n",
    "            df.dropna(axis=1, how='all', inplace=True)\n",
    "            df.drop(columns=['bb_rowwise', 'con_rowwise'], inplace=True)\n",
    "            df.fillna(\"\", inplace=True)\n",
    "\n",
    "            # renaming the columns into standard format\n",
    "            df.rename(columns={\n",
    "                \"Particular\": \"particular\",\n",
    "                'Unit_price': 'unit_price',\n",
    "                'Quantity': 'quantity',\n",
    "                'After_discount_amount': 'ad_amount',\n",
    "                'Before_discount_amount': 'bd_amount',\n",
    "                'Discount': 'discount'\n",
    "                }, inplace=True)\n",
    "\n",
    "            pageid = imgname.split('_')[1].split('.')[0]\n",
    "            # get the request wise postqc data\n",
    "            req_wise = postqc[postqc['txn_id']==txn_id]\n",
    "            page_wise = req_wise[req_wise['page_id']==int(pageid)]\n",
    "            page_wise = page_wise.sort_values(by='line_id')\n",
    "            max_lines = page_wise.shape[0]\n",
    "            new_line_id = range(1, max_lines + 1)\n",
    "            page_wise['line_id'] = new_line_id \n",
    "            page_wise.rename(columns={\n",
    "                'line_id': 'line_id',\n",
    "                \"particular\": \"particular\",\n",
    "                'unit_price': 'unit_price',\n",
    "                'quantity': 'quantity',\n",
    "                'bd_amount': 'bd_amount',\n",
    "                'discount': 'discount',\n",
    "                'ad_amount': 'ad_amount'\n",
    "                }, inplace=True)\n",
    "            try:\n",
    "                table_type = page_wise['table_type'].values[0]\n",
    "            except:\n",
    "                table_type = 'Detail'\n",
    "\n",
    "            if len(page_wise) > 0:\n",
    "                common_column = list(page_wise.columns.intersection(df.columns))\n",
    "                common_column.append('line_id')\n",
    "                common_column.append('tl_x')\n",
    "                common_column.append('tl_y')\n",
    "                common_column.append('br_x')\n",
    "                common_column.append('br_y')\n",
    "                # common_column.append('table_type')\n",
    "                page_wisee = page_wise[common_column]\n",
    "                page_wisee.rename(columns={\n",
    "                    'tl_x': 'tl_x',\n",
    "                    'tl_y': 'tl_y',\n",
    "                    'br_x': 'br_x',\n",
    "                    'br_y': 'br_y'\n",
    "                    }, inplace=True)\n",
    "                page_wisee['tl_x'] = page_wisee['tl_x'].astype(float).round().astype(int)\n",
    "                page_wisee['tl_y'] = page_wisee['tl_y'].astype(float).round().astype(int)\n",
    "                page_wisee['br_x'] = page_wisee['br_x'].astype(float).round().astype(int)\n",
    "                page_wisee['br_y'] = page_wisee['br_y'].astype(float).round().astype(int)\n",
    "\n",
    "                colus = ['particular', 'unit_price', 'quantity', 'bd_amount', 'discount', 'ad_amount', 'limne_id']\n",
    "                for col in colus:\n",
    "                    if col in page_wisee.columns:\n",
    "                        if col == 'particular':\n",
    "                            page_wisee[col] = page_wisee[col].astype(str)\n",
    "                        elif col == 'line_id':\n",
    "                            page_wisee[col] = page_wisee[col].astype(float).round().astype(int)\n",
    "                        else:\n",
    "                            page_wisee[col] = page_wisee[col].astype(float)\n",
    "\n",
    "                # if (df.shape[0] != page_wisee.shape[0]) and (page_wise.current_state.values[0] == 'Extraction_Finished'):\n",
    "                #     return None\n",
    "\n",
    "            else:\n",
    "                page_flag = True\n",
    "                return None  # return None if no page-wise data\n",
    "\n",
    "        else:\n",
    "            page_flag = True\n",
    "            return None  # return None if no interim data\n",
    "\n",
    "        if page_flag:\n",
    "            return {\n",
    "                \"file_name\": f\"{image}\",\n",
    "                \"ground_truth\": json.dumps({\n",
    "                    'gt_parse': {'table': [], 'page_type': 'others', 'table_type': 'others'},\n",
    "                    'meta': {\n",
    "                        'version': '2.0.0',\n",
    "                        'split': f\"{splitt}\",\n",
    "                        'image_id': image,\n",
    "                        'image_size': {'width': width, 'height': height}\n",
    "                        },\n",
    "                    'valid_line': [],\n",
    "                    'roi': [],\n",
    "                    'repeating_symbol': [],\n",
    "                    'dontcare': []\n",
    "                })\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"file_name\": f\"{image}\",\n",
    "                \"ground_truth\": json.dumps({\n",
    "                    'gt_parse': {'table': page_wisee.to_dict('records'), 'page_type': 'BILL', 'table_type': table_type},\n",
    "                    'meta': {\n",
    "                        'version': '2.0.0',\n",
    "                        'split': f\"{splitt}\",\n",
    "                        'image_id': image,\n",
    "                        'image_size': {'width': width, 'height': height}\n",
    "                        },\n",
    "                    'valid_line': [],\n",
    "                    'roi': [],\n",
    "                    'repeating_symbol': [],\n",
    "                    'dontcare': []\n",
    "                })\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(e, image)\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    max_threads = 12\n",
    "    \n",
    "    for splitt in ['validation']:\n",
    "        metadata_accumulator = []\n",
    "        image_folder = os.path.join(dataset_path, splitt)\n",
    "        image_files = [f for f in os.listdir(image_folder) if f != 'metadata.jsonl']\n",
    "\n",
    "        with ProcessPoolExecutor(max_workers=max_threads) as executor:\n",
    "            results = list(tqdm(executor.map(process_image, image_files), total=len(image_files)))\n",
    "        \n",
    "        with jsonlines.open(save_path + f\"{splitt}_metadata.jsonl\", 'a') as wr:\n",
    "            for result in results:\n",
    "                if result:\n",
    "                    wr.write(result)\n",
    "\n",
    "        print(f\"{splitt.capitalize()} images processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "images = os.listdir('/home/balajic/workdir/donut_training_pipeline/multipager_donut_original/validation')\n",
    "with open('//home/balajic/workdir/donut_training_pipeline/train.txt', 'w') as fp:\n",
    "    fp.write(str(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5201"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "setup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
